---
layout: default
---

# Lidar 传感器

## 1. 介绍

欢迎来到这节关于激光雷达技术的课程。如果没有激光雷达传感器，我们很可能看不到完全自动驾驶汽车成为现实。在第一章中，我们将首先从激光雷达在自动驾驶中的一般作用开始。您将了解自动驾驶的各个级别，简要介绍相机、激光雷达和雷达，我们还将讨论您需要考虑的传感器选择标准。

**自动驾驶水平分级**

在我们分析一组选定的自动驾驶汽车原型和它们各自的传感器之前，让我们先来看看如何定义自动驾驶——因为并非所有的自动驾驶汽车和驾驶员辅助系统都是平等的。下图显示的是SAE定义的“自动驾驶水平”。

![Levels of Automated Driving - L0 is no automation, L1 is driver assistance, L2 is partial automation, L3 is conditional automation, L4 is high automation, and L5 is full automation. See https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety for descriptions.](https://qqsj789.github.io/img//c1-2-img2-16348718037144.jpg)

多年来，驾驶辅助系统(ADAS)，如前向碰撞预警、刹车或自适应巡航控制，是唯一能够在某些驾驶情况下(无论是安全还是舒适)至少实现单一车辆功能自动化的系统。

特斯拉是全球首批向市场推出意味着高度自动化的自动驾驶系统的公司之一。然而，在SAE的图表中，该系统的第一个版本仅为2级，这意味着驾驶员必须保持参与，并应随时监控环境。

到第3级是一个很大的步骤，因为驾驶员不再需要监控环境，即使他必须能够在任何时候收回控制权。从法律的角度来看，这意味着驾驶任务的责任是由汽车和制造商。这就是为什么我们没有看到大量的商用车辆配备3级系统。一些制造商已经宣布了这样的系统，但我们还没有在市场上找到它们。原因有三

- 这样的系统必须建立得足够可靠，使错误决策最小化。工程师通常通过在汽车上增加大量传感器来解决这个问题，这使得这种系统(非常)昂贵。
- 对事故引发的诉讼的担心，导致系统的可用性故意降低，例如限制驾驶速度或场景(例如，仅在车速低于60公里时，有清晰可见车道标志的高速公路上出现交通堵塞)。
- 司机随时可以控制车辆是无法保证的。在许多情况下，由于人类的反应时间和警觉性水平，这是不可能的。

基于这种推理，许多专家认为，3级系统只是向运行在4级和5级的更先进系统的过渡步骤。在这些级别，车辆能够执行所有驾驶任务，而不需要驾驶员控制。显然，这样的系统将需要强大的工程努力，以保证驾驶者和道路使用者的安全。然而，市场上有一些公司打算完全跳过第3和第4级，而试图直接进入第5级。众所周知的例子是Waymo和特斯拉。

**自动驾驶汽车传感器类型**

在自动驾驶中，有四种主要的目标检测传感器类型，它们被用于不同的组合监视车辆周围。让我们简要回顾一下这些类型，以便您能够更好地理解激光雷达传感器在今后课程中的作用。

![Sensors mounted on a Waymo car, with radar, lidar and camera visible.](https://qqsj789.github.io/img//c1-2-img3-16348718671656.jpg)

**摄像头**

摄像头在许多汽车上已经很常见了，用于各种用途，比如车辆和行人检测、车道检测、道路标志识别，或者只是为了向司机展示后方的景象。相机属于被动传感器，这意味着它们将物体反射的环境光转换成二维图像。

![Udacity front-facing camera](https://qqsj789.github.io/img//c1-2-img1-16348715046072.png)

自动驾驶汽车在许多需要执行的功能上都广泛地依赖于摄像头，这远远超出了障碍物检测(如高精度测绘和定位)的范围。

然而，在交通中有许多情况下，物体反射的光不足以进行稳定的检测(例如在夜间光线不好的地区，在大雨中，在阳光直射下)。在这种情况下，需要其他类型的传感器，它们不依赖于环境条件。

![A vehicle driving at night with its headlights on in the rain](https://qqsj789.github.io/img//c1-2-img4-16348719329478.jpg)

**雷达**

雷达和照相机一样，今天的许多车辆已经配备了一个或多个雷达传感器。这种传感器类型最常用于驾驶辅助系统，如“自适应巡航控制”或“自动紧急制动”。

雷达是主动传感器，因为它们发射电磁波，这种电磁波会从具有特定属性的物体(如金属)反射回来。根据信号的运行时间，雷达传感器可以非常准确地估计距离。此外，基于一种叫做“多普勒效应”的物理原理，雷达可以通过评估回波信号的频移来测量运动物体的速度，频移与物体的速度成正比。

雷达传感器的主要优点之一是它们能够在几乎所有天气条件下可靠地工作。然而，雷达的空间分辨率非常低，这就是为什么它们不适合非常精确地测量物体的尺寸。此外，金属含量很少或没有的物体(如行人)不会产生强烈的返回信号，而且很难从背景噪音中分离出来。

汽车雷达传感器通常工作在两个频段之一:24GHz雷达传感器用于近程应用，具有更大的开启角度，而77GHz传感器用于窄锥的远程传感。

![Long-range and short-range radar cones - the short range goes to 30m at 24 GHz with a wider field of view, with long-range at 150m and 77 GHz, with a narrow field of view.](https://qqsj789.github.io/img//sdcnd-c1-2-img5-163487208512810.png)

**激光雷达**

激光雷达也属于主动传感器。它的基本原理是发射激光束并测量光从物体反射回传感器所需的时间。目前在自动驾驶汽车上最常用的激光雷达类型是安装在车顶上的设备，可以以360度的弧线快速旋转，每秒产生数千个测量值。

这种旋转激光雷达传感器可以创建非常精确的周围环境的3D点地图，从而可以检测车辆、行人、自行车和其他障碍物。这种传感器类型也被称为扫描激光雷达，因为它需要移动它的部件以逐步光栅(“扫描”)的视野。

![An older roof mount of lidars and cameras from Stanford's "Stanley" self-driving car](https://qqsj789.github.io/img//c1-4-img1-163487741743024.png)

然而，目前可用的传感器类型有一个显著的缺点:根据型号和性能，它们通常要花费几千美元。尽管工程师们一直在努力降低成本，但激光雷达仍然是自动驾驶汽车中最昂贵(也是体积最大)的传感器。

目前，激光雷达行业正致力于三个核心问题:

- 降低单价
- 减小封装尺寸
- 增加传感范围和分辨率

作为扫描激光雷达的替代方案，还有非扫描传感器，也被称为闪光激光雷达。“闪光”一词指的是视场完全由激光光源照亮，就像带有闪光灯的相机一样，同时一组光电探测器同时接收反射的激光脉冲。

闪光激光雷达传感器没有任何移动部件，这就是为什么它们抗振动，而且比扫描激光雷达传感器的包装尺寸要小得多。这种传感器类型的缺点是范围有限，与安装在屋顶的激光雷达类型相比，视野相对狭窄。

在自动驾驶车辆中，扫描和非扫描激光雷达均用于观察车辆周围的不同区域：车顶安装的扫描激光雷达可生成360度视图，直到范围约为80-100m，而非扫描激光雷达传感器（通常安装在四个角）观察顶部安装的传感器为盲板的区域内车辆的直接附近。

**其他传感器**

除了照相机、雷达和激光雷达之外，还有其他传感器类型，如超声波传感器（自20世纪90年代以来广泛用于停车应用）或立体相机（有时也称为伪激光雷达）。但是，这些传感器超出了本课程的范围。从传感器融合的角度来看，最好将摄像头传感器与激光雷达或雷达或两者结合起来，以获得车辆周围环境的可靠和准确重建。

## 2. 传感器选择标准

为自动驾驶车辆选择合适的传感器组是一项微妙的任务，因为您需要平衡从可靠性到成本的一系列因素，以便您的公司能够确定最佳点并选择最佳传感器组。正如您在上一节中了解到的，正在讨论哪些传感器组合是实现完全（甚至部分）自治所必需的。在本节中，您将了解传感器选择标准以及相机、激光雷达和雷达在每个标准方面的差异。

以下将简要讨论最典型的选择标准。

**范围**：激光雷达和雷达系统可以探测距离从几米到200米以上的物体。许多激光雷达系统很难在非常近的距离内探测到物体，而雷达可以在不到一米的距离内探测到物体，这取决于系统类型（长距离、中距离或短距离）。单摄像头无法可靠地测量到物体的公制距离-这只有通过对世界的性质（例如，平面路面）做出一些假设才能实现。另一方面，立体摄像机可以测量距离，但只能测量约80米的距离，因此精度会大大降低。

**空间分辨率**：由于发射的红外激光波长较短，激光雷达扫描的空间分辨率约为0.1°。这允许进行高分辨率3D扫描，从而对场景中的对象进行特征化。另一方面，雷达不能很好地分辨小特征，尤其是随着距离的增加。相机系统的空间分辨率由光学元件、图像上的像素大小及其信噪比决定。当小物体发出的光线扩散到图像传感器上的几个像素（模糊）时，小物体的细节就会丢失。此外，当几乎不存在用于照亮对象的环境光时，随着图像传感器的噪声级增加，对象细节被叠加，空间分辨率降低。

**黑暗中的鲁棒性**：雷达和激光雷达在黑暗中都具有极好的鲁棒性，因为它们都是主动传感器。虽然激光雷达系统的白天性能非常好，但它们在夜间的性能甚至更好，因为没有可能干扰红外激光反射探测的环境阳光。另一方面，摄像机在夜间的探测能力非常低，因为它们是依赖环境光的被动传感器。尽管图像传感器的夜间性能有所提高，但在三种传感器类型中，它们的性能最低。

**在雨、雪、雾中的鲁棒性**：雷达传感器的最大优点之一是在恶劣天气条件下的性能。它们不会受到雪、大雨或空气中任何其他障碍物（如雾或沙粒）的显著影响。作为一种光学系统，激光雷达和相机容易受到恶劣天气的影响，其性能通常会随着恶劣天气程度的增加而显著下降。

**物体分类**：摄像机擅长对车辆、行人、速度标志等物体进行分类。这是摄像系统的主要优势之一，人工智能的最新进展更强调了这一点。使用高密度3D点云的激光雷达扫描也可以实现一定程度的分类，尽管其对象多样性不如照相机。雷达系统不允许进行很多物体分类。

**感知二维结构**：摄像机系统是唯一能够解释二维信息（如速度标志、车道标志或交通灯）的传感器，因为它们能够测量颜色和光强度。这是摄像头相对于其他传感器类型的主要优势。

**测量速度**：雷达可以利用多普勒频移直接测量物体的速度。这是雷达传感器的主要优点之一。激光雷达只能通过连续的距离测量来近似速度，这使得它在这方面的精确度较低。尽管相机无法测量距离，但可以通过观察图像平面上物体的位移来测量碰撞时间。本课程稍后将使用此属性。

**系统成本**：近年来，雷达系统已广泛应用于汽车行业，目前的系统高度紧凑且价格合理。单声道相机也是如此，在大多数情况下价格远低于100美元。由于硬件成本的增加和市场上数量的显著减少，立体相机的价格更高。激光雷达在过去几年中越来越受欢迎，尤其是在汽车行业。由于技术进步，其成本已从75000多美元降至5000美元以下。许多专家预测，未来几年，激光雷达模块的成本可能会降至500美元以下。

**包装尺寸**：雷达和单摄像头都可以很好地集成到车辆中。在某些情况下，立体摄像机体积庞大，因此很难将其集成到挡风玻璃后面，因为它们有时可能会限制驾驶员的视野。激光雷达系统有各种尺寸。360°扫描激光雷达通常安装在屋顶顶部，因此非常清晰可见。工业向更小的固态激光雷达系统的转变将在不久的将来大大缩小激光雷达传感器的系统尺寸。

**计算要求**：激光雷达和雷达几乎不需要后端处理。虽然相机是一种经济高效且易于使用的传感器，但它们需要大量的处理才能从图像中提取有用的信息，这增加了系统的总体成本。

|            | Range measurement | Robustness in darkness | Robustness in rain, snow, or fog | Classification of objects | Perceiving 2D Structures | Measure speed / TTC | Package size |
| :--------: | :---------------: | :--------------------: | :------------------------------: | :-----------------------: | :----------------------: | :-----------------: | :----------: |
| **Camera** |         -         |           -            |                -                 |            ++             |            ++            |          +          |      +       |
| **Radar**  |        ++         |           ++           |                ++                |             -             |            -             |         ++          |      +       |
| **Lidar**  |         +         |           ++           |                +                 |             +             |            -             |          +          |      -       |



## 3. Waymo激光雷达技术规范

在本节中，您将简要了解我们将在整个课程中使用的激光雷达传感器。我们将简要介绍Waymo无人驾驶车辆，以及主要360°激光雷达的一些技术规格。本节的目的是让您对本课程目标检测部分将要处理的数据有一个第一印象。

在最后一节中，我们介绍了自动驾驶中使用的三种主要传感器类型。此外，您还了解到，关于激光雷达作为传感器组一部分的必要性，正在进行激烈的讨论。Waymo是激光雷达技术的主要倡导者之一，它总共使用五个激光雷达传感器来装备无人驾驶车辆。

如下图所示，Waymo还使用了多个摄像头以及雷达传感器进行前/后监控。

![The Waymo sensor suite - on the left is a picture of the vehicle, while the right is a diagram, showing multiple lidar, radar and cameras at different locations.](https://qqsj789.github.io/img//sdcnd-c1-4-img3-163487797204628.png)

激光雷达传感器可分为两大类：

**周边激光雷达**：该传感器的垂直视野范围为-90°至+30°，范围限制为0-20m。范围限制由Waymo施加在数据上，仅存在于Waymo开放数据集中。可以假设实际传感器范围明显更大。周边激光雷达位于Waymo无人驾驶车辆的前部和后部以及左/右前角。有趣的是，周界激光雷达出售给了与Waymo没有直接竞争的公司，其品牌为Laser Bear Honeycomb。下图显示了左前激光雷达传感器生成的3d点云：

![Waymo Front-Left LiDAR 3D Point-Cloud - shows various points out to the front left of the vehicle.](https://qqsj789.github.io/img//c1-3-img4-163487443125320.png)

360激光雷达：顶部激光雷达的垂直视野范围为-17.6°至+2.4°，数据集中的范围限制为75m。该激光雷达绕其垂直轴旋转，并在车辆360°圆周上生成高分辨率3d图像。在本课程中，我们将使用这种传感器类型的数据来检测车辆。下图显示了该激光雷达传感器在鸟瞰视角下生成的3d点云：

![Shows a point cloud that looks like a bird's eye view of points surrounding all angles of the vehicle](https://qqsj789.github.io/img//sdcnd-c1-4-img4-163487824472730.png)

这里有两个方面值得注意：（1）相邻扫描线之间的距离随着距离的增加而增加，（b）车辆直接圆周上的区域不包含任何3d点。通过观察传感器车辆设置的几何结构，可以很容易地解释这两个观察结果：

![A graphic showing a blind spot for the top lidar just in front of the vehicle, with widening beam distances further out](https://qqsj789.github.io/img//sdcnd-c1-4-img6-163487879812434.png)

在车辆正前方，由于车辆遮挡了激光束，因此存在较大的感知间隙（“盲点”）。此外，可以看出，由于激光二极管垂直放置的固定角度，相邻光束之间的间隙随着距离的增加而增大。

本节的目的是让您从顶层了解Waymo传感器套件的一般设置。在本课程的下一节中，我们将更深入地研究激光雷达技术的技术特性。现在让我们看一看起始代码，您可以使用它加载、可视化和处理激光雷达数据。

## 4. Waymo数据集中的帧结构

在本节中，您将学习如何在Waymo Open数据集中构造帧。此外，您还将熟悉从帧中提取数据元素所需的代码，例如特定的相机图像或激光雷达数据。

**如何访问帧**

Waymo数据集将信息存储在.tfrecord文件中。在本课程中，您将使用三个具有不同内容的文件（车辆数量、杂乱情况、各种驾驶条件等）。

所有序列包含约200个独立帧，具有以下顶级结构：

- ```
  |-- LaserName
  |-- CameraName
  |-- RollingShutterReadOutDirection
  |-- Frame
  |-- Label
  ```

当实际传感器数据存储在框架结构中时，我们可以使用LaserName和CameraName来选择我们想要访问的特定传感器。对于每个激光器和相机，框架下嵌套有一个子分支，其中包含以下项目：

```
-- Frame
   |-- images
   |-- Context
   |   |-- name
   |   |-- camera_calibrations
   |   |-- laser_calibrations
   |   |-- Stats
   |   |-- laser_object_counts
   |   |-- camera_object_counts
   |   |-- time_of_day
   |   |-- location
   |   |-- weather
   |-- timestamp_micros
   |-- pose
   |-- lasers
   |-- laser_labels
   |-- projected_lidar_labels (same as camera_labels)
   |-- camera_labels
   |-- no_label_zones
```

**访问相机数据**

要访问特定摄像头，可以使用以下代码：

```python
camera_name = dataset_pb2.CameraName.FRONT
image = [obj for obj in frame.images if obj.name == camera_name][0]
```

图像分支具有以下结构：

```
|-- Frame
   |-- images ⇒ one branch for each entry in CameraName
      |-- name (CameraName)
      |-- image
      |-- pose
      |-- velocity (v_x, v_y, v_z, w_x, w_y, w_z)
      |-- pose_timestamp
      |-- shutter
      |-- camera_trigger_time
      |-- camera_readout_done_time
```

摄像机数据结构被命名为image可能会引起一些混淆，即使实际图像只是其中许多项中的一项（也称为image）。

如果要访问和显示相机图像，可以使用以下代码：

```python
from PIL import Image
import io

# convert the image into rgb format
image = np.array(Image.open(io.BytesIO(camera.image)))
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# resize the image to better fit the screen
dim = (int(image.shape[1] * 0.5), int(image.shape[0] * 0.5))
resized = cv2.resize(image, dim)

# display the image 
cv2.imshow("Front-camera image", resized)
cv2.waitKey(0)
```

此代码生成以下输出：

![A front-camera image, with the car driving down a road with other cars, trees, bushes and a wall to the right visible](https://qqsj789.github.io/img//c1-4-img5-163487846876932.png)

**访问激光雷达数据**

现在，让我们使用以下代码以类似的方式访问激光雷达数据：

```python
lidar_name = dataset_pb2.LaserName.TOP
lidar = [obj for obj in frame.lasers if obj.name == lidar_name][0]
```

在调试器中检查激光雷达分支时，您会发现它具有以下结构：

```
-- lasers ⇒ one branch for each entry in LaserName
        |-- name (LaserName)
        |-- ri_return1 (RangeImage class)
            |-- range_image_compressed
            |-- camera_projection_compressed
            |-- range_image_pose_compressed
            |-- range_image
        |-- ri_return2 (same as ri_return1)
```

您可能想知道，为什么不能像预期的那样直接访问点云。原因是，在Waymo数据集中，激光雷达测量值存储在称为距离图像的结构中。您将很快了解有关此概念的更多信息，以及如何从中生成点云。然而，在我们深入研究之前，您需要更好地理解激光雷达技术背后的物理原理，这将是本课程下一章的主要目标。

但首先，让我们通过查看框架结构中的一些剩余条目来结束本章：

```
|-- Frame
        |-- Context
            |-- name
            |-- camera_calibrations ⇒ one branch for each entry in CameraName
            |-- laser_calibrations ⇒ ⇒ one branch for each entry in LaserName
            |-- Stats
            |-- laser_object_counts
            |-- camera_object_counts
            |-- time_of_day
            |-- location
            |-- weather
        |-- timestamp_micros
        |-- laser_labels ⇒ one branch for each entry in LaserName
        |-- camera_labels ⇒ one branch for each entry in CameraName
```

在上下文分支中，有大量有用的数据条目，其中一些我们将在本课程中使用。例如，通过激光校准，您可以检索单个激光LED的光束倾斜以及外部校准。

使用以下代码，您可以检索顶部激光雷达传感器的校准数据：

```python
lidar_name = dataset_pb2.LaserName.TOP
calib_lidar = [obj for obj in frame.context.laser_calibrations if obj.name == lidar_name][0]
```

使用调试器检查校准数据会产生以下结果：

```python
vfov_rad = calib_lidar.beam_inclination_max - calib_lidar.beam_inclination_min
print(vfov_rad*180/np.pi)
--> 20.360222720319797
```

从描述Waymo数据集的论文中，我们可以收集20度的垂直视野，这与上述输出一致。

我们可以从外部校准矩阵中了解到，顶部激光雷达位于距离车辆坐标系原点+1.43m处，高度为+2.184m。这些值可以在数据结构calib_lidar.extrinsic.transform中找到：
$$
\small \begin{bmatrix} - 0.8526719509207284 & -0.5224378704141576 & -0.0030357322277815815 & 1.43 \\ 0.5224451202853144 & -0.8526692389088371 & -0.0025030598650299957 & 0.0 \\ 0.0012807822227881296 & -0.0037202924272838555 & 0.9999922594806188 & 2.184 \\ 0.0 & 0.0 & 0.0 & 1.0 \end{bmatrix}
$$
最后，子分支“laser_labels”包含当前帧中存在的所有手动标记的真实对象的列表。我们将在“激光雷达数据可视化”一节中讨论这些问题。

请记住，本节是关于获取帧数据结构的概述。我们将在本课程后面的相关章节中重温许多单独的元素。在下一章中，我们将研究激光雷达传感器的物理特性以及目前市场上可用的一些型号。

## 5. 激光雷达技术特性

**激光雷达简史**

激光雷达技术已经存在了很长一段时间。这一基本思想最初是由爱尔兰物理学家爱德华·辛格（Edward H.Synge）在20世纪30年代提出的。在随后的几十年中，出现了一些最早的应用，如1969年阿波罗11号上的月球测距实验或2000年创建的第一个考古遗址数字高程模型。2005年，斯坦福车队赢得了DARPA大挑战赛，这是一场在莫哈韦沙漠进行的自动驾驶车辆之间的比赛，使用了一系列病态线扫描仪。

![Stanford "Stanley" in the DARPA Grand Challenge](https://qqsj789.github.io/img//sdcnd-c1-4-img16-163487935781738.png)

两年后，在2007年，斯坦福球队首先是完成DARPA城市挑战，这是一座嘲弄城市的乔治空军基地的比赛，使用旋转360°Velodyne Lidar传感器作为其中之一主要传感设备。

![Stanford Junior in the DARPA Urban Challenge, showing various radar and lasers](https://qqsj789.github.io/img//c1-3-img3-163487439357418.png)

自那时起，随着自动驾驶的出现，激光雷达技术得到了重大改进，使其成为自动车辆传感器套件的基石之一。但是，在我们进一步了解细节之前，让我们回顾一下激光雷达的基本工作原理。

**激光雷达工作原理**

目前最常用的激光雷达传感器称为“脉冲激光雷达”。它是一个由激光源和接收器组成的系统，激光源向场景发射短时间的密集激光束。当光束击中物体时，一部分激光被折射回激光雷达传感器，并可被接收器检测到。根据激光的飞行时间，可使用以下等式计算到目标的距离R：
$$
R = \frac{1}{2n}\cdot c \Delta t
$$
式中，c是真空中的光速，n是传播介质的折射率（对于空气，n可假定为1.0）。

在查看示例之前，让我们先讨论典型激光雷达传感器的组件：

![LiDAR sensor components. The laser beam is made of a source and amplifier, which then goes to both a photo-detector to start signal and to an emitter (made of a beam scanner and transmitter optics). Once the beam hits a target, it comes back to a receiver (made of receiver optics and a photodetector), which stops the signal on the timing device, and then calculates range.](https://qqsj789.github.io/img//c1-3-img1-163487422500314.png)

在示意图中，您可以看到主要部件“激光源”、“发射器”、“接收器”和“计时器”。首先，激光源产生几皮秒或纳秒量级的极短脉冲。激光脉冲随后被放大器放大，然后在光束扫描仪和发射光学器件的帮助下被引导到大气中。当每个脉冲扫描感兴趣的区域时，它被引导到传感器视野内的特定位置。到达接收器透镜的后向散射脉冲能量的一部分通过接收器光学元件收集，然后放大并转换为电压信号。

为了精确测量光束发射和检测之间的时间，需要一个非常精确的时钟。从下图可以看出，前沿阈值技术用于电压信号，以检测激光脉冲返回时的时间瞬间。

![LiDAR beam start and stop pulse](https://qqsj789.github.io/img//c1-3-img5-163487553163922.png)

可达到的距离分辨率与定时装置的分辨率成正比。可以假设时间间隔测量的典型分辨率值在0.1 ns范围内，这导致范围分辨率为1.5 cm。

探测目标的最大距离主要取决于激光束在大气中传播时的能量损失。返回能量越低，环境噪声越高，接收器越难检测到清晰的侧翼。信号能量和背景噪声之间的比率由信噪比（SNR）来描述，信噪比（SNR）用于显示从不同距离的目标返回的多个信号。

![LiDAR signal returns at varying distances, with different frequencies by distance.](https://qqsj789.github.io/img//c1-3-img2-163487434696716.png)

从图中还可以看出，信号峰值与目标距离成比例变平，这是由于缺少光束相干性造成的。这种效应称为“光束发散”，它与激光的波长λ成正比。例如，对于λ=1550nm的激光雷达，由于光束发散，横向上的最小可分辨特征尺寸约为4cm，距离为100米。仅作比较，波长为λ=0.3cm的77GHz雷达传感器，在相同距离下的最小可分辨特征尺寸为2m。

此外，从图中可以看出，信号峰值SNR随着距离的增加而降低，这是由大气中阻碍激光路径的粒子（如水或灰尘）造成的。本文深入分析了雾和雨对激光雷达性能的影响。

提高信噪比有两种基本解决方案，即（a）增加激光能量和（b）提高接收机在噪声存在时检测微弱信号的灵敏度。虽然（a）受到眼睛安全法规的限制，但方法（b）增加了接收器电子设备的复杂性。

关于最大范围的另一个需要考虑的因素是信号模糊性，即在每个时间点，只有一个激光脉冲“在飞行”，以便接收到的脉冲可以明确地与先前发射的脉冲相关联。

尽管存在上述限制，飞行时间脉冲激光雷达系统是（目前）最常选择用于自动驾驶车辆的类型，这主要是因为其结构简单，即使在具有挑战性的环境条件下也能在室外良好工作。

其他飞行时间测量方法有雷达和超声波。在这三种ToF技术中，激光雷达提供了最高的角度分辨率，因为它的光束发散明显较小。因此，它可以更好地分离场景中的相邻对象，如下图所示：

![LiDAR and radar beam divergence - the lidar could hit the back of a vehicle for instance, while radar hits the side of the same vehicle, showing a divergence in detection.](https://qqsj789.github.io/img//c1-2-img6-163487218403412.png)

然而，对于目前可用的激光雷达系统，可探测物体的最大距离仍低于雷达，这限制了激光雷达作为主要传感设备的速度（例如，在高速公路场景中）。

**激光雷达方程**

在最后一节中，我们介绍了飞行时间脉冲激光雷达的基本工作原理。您现在知道，影响激光雷达系统检测质量的两个挑战性因素是（a）光束相干性和（b）信噪比。在本节中，您将简要介绍所谓的“激光雷达方程”，该方程将返回到接收器的激光束的功率与许多因素联系起来，例如发送器中的传输功率、目标反射率以及光学元件和目标之间的大气条件。

请注意，根据应用领域的不同，激光雷达方程不是“一个”，而是几个。在汽车传感中，最常用的公式如下：
$$
P(R) = P_0\cdot \rho \cdot \frac{A_0}{\pi \cdot R^2}\cdot \eta_0 \cdot e^{-2\gamma R}
$$
让我们快速查看各个参数及其对自动驾驶的影响：

- **P（R）**：接收功率->可以很容易地看出，我们希望在高信噪比（SNR）下最大化该表达式，从而稳定准确地检测反射激光脉冲。
- *P*0：峰值传输功率->放大器中使用的功率越多，返回接收器的光子越多。控制该参数的限制因素有两个，即眼睛安全规定和功耗。
- ρ：目标反射率->目标表面的反射性越强，返回接收器的光子越多。下图显示了行人人体模型佩戴的一系列棉花样品的反射率：![LiDAR beam start and stop pulse, showing many different wavelengths and reflectances.](https://qqsj789.github.io/img//c1-4-img7-163487911976236.png)

从图中可以看出，不同样品之间的反射率差异很大（每种棉花类型由一种单独的颜色表示）。此外，反射率随发射激光的波长而变化：对于大多数材料，在1000nm左右的波长下，反射率在50%到70%之间，在2500nm左右，反射率降低到20%到40%左右。没有功率损耗的完美反射显然是100%。

- A_0：接收器孔径区域 - >与经典摄影一样，孔径的尺寸直接影响通过通过撞击接收器的光量。孔径越大，返回光子的数量越高。
- *η*0 ：接收器光学的透射系数->当光子通过非真空介质时，由于其飞行路径中的障碍物而散射。透射系数表示散射的程度，返回的光子在通过光学器件的路径上经历散射。丢失的光子越多，到达接收器的光子就越少，这会降低信号电压水平，从而降低信噪比。
- \γ：大气消光系数->与光学传输系数类似，大气消光系数描述了由于与大气中的空气颗粒（如水分子或灰尘）碰撞而导致的光子损失量。空气中的粒子越多，返回接收器的光子就越少，这也会降低信噪比。

基于这些参数，您可以了解从激光束生成到光子检测的各种组件如何影响接收功率，从而影响信噪比。

**多信号返回**

您可能已经注意到上一节关于Waymo框架结构的条目ri_return2。该数据结构保存激光雷达光束的“第二次返回”，该光束在第一次（即主）返回后到达。由于激光束的直径有限，很可能只有一部分被物体反射，特别是当激光击中深度不连续点附近时，如车辆边缘。在下图中，激光束首先击中前面车辆左侧，产生第一次返回。然而，由于目标仅被光束部分击中，一小部分激光继续在其路径上，直到它最终击中另一个目标，在这种情况下，另一个目标是另一辆车。由于距离较大，返回信号通常比主返回信号弱，在主返回信号之后到达接收器，因此被称为“第二返回”。

![Primary and secondary LiDAR returns](https://qqsj789.github.io/img//c1-4-img2-163487746428626.png)

在本课程中，我们将不使用Waymo数据集中的ri_return2结构，但原则上，它可以用来检测更精确的对象边界，类似于计算机视觉中使用的边缘检测技术。

## [回首页](../index.html) 